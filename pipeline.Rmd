---
title: "Compilation Report | Corpus des Deutschen Bundesrechts (C-DBR)"
author: Seán Fobbe
geometry: margin=3cm
fontsize: 11pt
papersize: a4
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
    pandoc_args: --listings
    includes:
      in_header: tex/Preamble_DE.tex
      before_body: [temp/Definitions.tex, tex/Titlepage_Compilation.tex]
bibliography: temp/packages.bib
nocite: '@*'
---



```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = TRUE,
                      message = TRUE,
                      collapse = TRUE,
                      comment = "#>")
```




```{r, results = "asis", echo = FALSE}
cat(readLines("README.md")[-1],
    sep = "\n")
```



# Packages laden


```{r}

library(targets)
library(tarchetypes)
library(RcppTOML)
library(future)
library(data.table)
library(quanteda)
library(knitr)
library(kableExtra)
library(igraph)
library(ggraph)

tar_unscript()
```



# Vorbereitung

## Definitionen

```{r}

## Datum
datestamp <- Sys.Date()
print(datestamp)

## Datum und Uhrzeit (Beginn)
begin.script <- Sys.time()

## Konfiguration
config <- RcppTOML::parseTOML("config.toml")
print(config)


# Analyse-Ordner
dir.analysis <- paste0(getwd(),
                       "/analysis")


```


## Aufräumen

Löscht Dateien im Output-Ordner, die nicht vom heutigen Tag sind.


```{r}

unlink(grep(datestamp,
            list.files("output",
                       full.names = TRUE),
            invert = TRUE,
            value = TRUE))


```



## Ordner erstellen

```{r}

#unlink("output", recursive = TRUE)
dir.create("files", showWarnings = FALSE)
dir.create("output", showWarnings = FALSE)
dir.create("temp", showWarnings = FALSE)

dir.create(dir.analysis, showWarnings = FALSE)

```



## Vollzitate statistischer Software schreiben

```{r}
knitr::write_bib(renv::dependencies()$Package,
                 "temp/packages.bib")
```




# Globale Variablen


## Packages definieren

```{targets global-packages, tar_globals = TRUE}

tar_option_set(packages = c("tarchetypes",
                            "RcppTOML",     # TOML-Dateien lesen und schreiben
							"testthat",     # Unit Tests
                            "fs",           # Verbessertes File Handling
                            "zip",          # Verbessertes ZIP Handling
                            "httr",         # HTTP-Werkzeuge
							"xml2",         # XML-Extraktion
                            "rvest",        # HTML-Extraktion
                            "knitr",        # Professionelles Reporting
                            "kableExtra",   # Verbesserte Kable Tabellen
                            "pdftools",     # Verarbeitung von PDF-Dateien
                            "ggplot2",      # Datenvisualisierung
							"ggraph",       # Visualisierung von Graphen
							"igraph",       # Analyse von Graphen
                            "scales",       # Skalierung von Diagrammen
                            "data.table",   # Fortgeschrittene Datenverarbeitung
                            "readtext",     # TXT-Dateien einlesen
                            "quanteda",     # Computerlinguistik
                            "future",       # Parallelisierung
                            "future.apply"))# Funktionen für Future

tar_option_set(workspace_on_error = TRUE) # Save Workspace on Error
tar_option_set(format = "qs")

```


## Konfiguration


```{targets global-config, tar_globals = TRUE}

datestamp <- Sys.Date()

config <- RcppTOML::parseTOML("config.toml")

dir.analysis <- paste0(getwd(),
                       "/analysis")

## Caption for diagrams
caption <- paste("Fobbe | DOI:",
                 config$doi$data$version)


## Prefix for figure titles
prefix.figuretitle <- paste(config$project$shortname,
                            "| Version",
                            datestamp)

## File prefix
prefix.files <- paste0(config$project$shortname,
                       "_",
                       datestamp)


if (config$cores$max == TRUE){
    fullCores <- future::availableCores() - 1
}


if (config$cores$max == FALSE){
    fullCores <- as.integer(config$cores$number)
	downloadCores <- as.integer(config$cores$download)
}

```




## Funktionen definieren

```{targets global-functions, tar_globals = TRUE}

lapply(list.files("functions", pattern = "\\.R$", full.names = TRUE), source)

```






## ZIP-Datei für Source definieren

```{targets global-sourcefiles, tar_globals = TRUE}

files.source.raw <-  c(system2("git", "ls-files", stdout = TRUE),
                       ".git")

```






# Pipeline: Konstruktion




## File Tracking Targets

Mit diesem Abschnitt der Pipeline werden Input-Dateien getrackt und eingelesen. Mit der Option \enquote{format = "file"} werden für Input-Dateien Prüfsummen berechnet. Falls sich diese verändern werden alle von ihnen abhängigen Pipeline-Schritte als veraltet markiert und neu berechnet.




### Source Code


Dies sind alle Dateien, die den Source Code für den Datensatz bereitstellen.

```{targets tar.file.source}
tar_target(files.source,
           files.source.raw,
           format = "file")

```


### Changelog

```{targets tar.file.changelog}
tar_target(changelog,
           "CHANGELOG.md",
           format = "file")
```


### Liste aller Variablen im Codebook

Die Variablen des Datensatzes, inklusive ihrer Erläuterung.


```{targets tar.file.varlist}
list(
    tar_target(file.var_codebook,
               "data/C-DBR_Variables.csv",
               format = "file"),
    tar_target(dt.var_codebook,
               fread(file.var_codebook))
)
```




## Download Targets

Es werden von \url{www.gesetze-im-internet.de} alle Rechtsakte im XML-, EPUB- und PDF-Format heruntergeladen und auf der Festplatte gespeichert. Die Document Type Definition (DTD) für die XML-Dateien wird ebenfalls archiviert.



### URLs für XML-Archive

```{targets tar.download.xmlurl}

tarchetypes::tar_age(url.xml,
                     f.links_xml(url = "https://www.gesetze-im-internet.de/gii-toc.xml"),
                     age = as.difftime(1, units = "days"))

```


### Tabelle der Dateinamen erstellen

```{targets tar.download.filenames}

tarchetypes::tar_age(dt.filenames,
                     f.html_landing_pages(url.xml,
                                          multicore = config$parallel$htmlLandingPages,
                                          cores = downloadCores),
                     age = as.difftime(1, units = "days"))

```

### Download Tabelle erstellen


```{targets tar.download.table}

tar_target(dt.download,
           f.download_table_make(dt.filenames = dt.filenames,
                                 url.xml = url.xml,
                                 xml.toc = "https://www.gesetze-im-internet.de/gii-toc.xml"))

```

### Konkordanzabelle erstellen


```{targets tar.download.conc}

tar_target(dt.conctable,
           f.conctable(dt.download = dt.download))

```



### Document Type Definition (DTD) herunterladen



```{targets tar.download.dtd}

tar_target(file.dtd,
           f.download("https://www.gesetze-im-internet.de/dtd/1.01/gii-norm.dtd",
                      paste0(prefix.files,
                             "_DE_XML_Document-Type-Definition_v1-01.dtd"),
                      dir = "output",
                      clean = FALSE),
           format = "file")

```



### XML (ZIP)-Archive herunterladen


```{targets tar.download.xmlzip}
tar_target(files.xmlzip,
                f.download(url = dt.download$url.xml,
                           filename = dt.download$title.xml,
                           dir = "files/xml_zip",
						   clean = TRUE,
						   random.order = FALSE,
						   multicore = config$parallel$downloadXML,
						   cores = downloadCores,
                           sleep.min = 0,
                           sleep.max = 0,
                           retries = 3,
                           retry.sleep.min = 1,
                           retry.sleep.max = 2,
                           timeout = config$download$timeout,
                           debug.toggle = FALSE,
                           debug.files = 500),
                format = "file")

```



### PDF-Dateien herunterladen


```{targets tar.download.pdf}
tar_target(files.pdf,
                f.download(url = dt.download$url.pdf,
                           filename = dt.download$title.pdf,
                           dir = "files/pdf",
						   clean = TRUE,
                           random.order = FALSE,
						   multicore = config$parallel$downloadPDF,
						   cores = downloadCores,
                           sleep.min = 0,
                           sleep.max = 0,
                           retries = 3,
                           retry.sleep.min = 1,
                           retry.sleep.max = 2,
                           timeout = config$download$timeout,
                           debug.toggle = FALSE,
                           debug.files = 500),
                format = "file")

```


### EPUB-Dateien herunterladen


```{targets tar.download.epub}
tar_target(files.epub,
                f.download(url = dt.download$url.epub,
                           filename = dt.download$title.epub,
                           dir = "files/epub",
						   clean = TRUE,
						   random.order = FALSE,
						   multicore = config$parallel$downloadEPUB,
						   cores = downloadCores,
                           sleep.min = 0,
                           sleep.max = 0,
                           retries = 3,
                           retry.sleep.min = 1,
                           retry.sleep.max = 2,
                           timeout = config$download$timeout,
                           debug.toggle = FALSE,
                           debug.files = 500),
                format = "file")

```



## Convert Targets

Dieser Abschnitt entpackt die ZIP-Dateien, erstellt ein Target mit allen XML-Dateien und konvertiert die PDF-Dateien in das TXT-Format.

### Entpacken


```{targets tar.convert.unzip}
tar_target(files.xml.all,
           f.tar_unzip(zipfiles = files.xmlzip,
                       exdir = "files/xml"),
           format = "file")

```

### XML-Dateien bestimmen


```{targets tar.convert.xmlfiles}
tar_target(files.xml,
           files.xml.all[grepl("\\.xml$", files.xml.all)],
           format = "file")

```



### PDF zu TXT konvertieren


```{targets tar.convert.txt}

tar_target(files.txt,
           f.tar_pdf_extract(x = files.pdf,
                             outputdir = "files/txt",
							 ignore.error = TRUE,
                             multicore = config$parallel$extractPDF,
                             cores = fullCores),
           format = "file")

```




## Parse Targets

Der Abschnitt zum Parsing extrahiert aus den XML-Dateien alle relevanten Normtexte und Metadaten. Es wird auch eine Netzwerk-Analyse der Struktur der Rechtsakte durchgeführt.


### Datensatz erstellen: Einzelnormen

```{targets tar.parse.normen}
tar_target(dt.normen,
           f.dt.einzelnormen(file.xml = files.xml,
                             multicore = config$parallel$parseEinzelnormen,
                             cores = fullCores))

```

### Datensatz erstellen: Rechtsakte (mit Text)

```{targets tar.parse.rechtsakte}
tar_target(dt.rechtsakte,
           f.dt.rechtsakte(dt.normen))

```


### Datensatz erstellen: XML-Metadaten

 Diese Datei unterscheidet sich von der Variante der "Rechtsakte (Metadaten)", weil sie auch Rechtsakte enthält, die ohne Text veröffentlicht wurden. Die Differenz betrifft etwa 1000 Rechtsakte, ist also erheblich.


```{targets tar.parse.xmlmeta}
tar_target(dt.meta,
           f.dt.meta(file.xml = files.xml,
                     multicore = config$parallel$parseMeta,
                     cores = fullCores))

```


### Netzwerk-Analyse

```{targets tar.parse.networks}
tar_target(files.network,
           f.network.analysis(files.xml = files.xml,
                              prefix.figuretitle = prefix.figuretitle,
                              caption = caption,
                              dir.out = "netzwerke",
                              multicore = config$parallel$parseNetworks,
                              cores = fullCores),
           format = "file")

```





## Enhance Targets

Hier werden vereinzelte Verbesserungen vorgenommen und weitere Variablen hinzugefügt. Schließlich werden die geprüften finalen Varianten erstellt.


### Variablen erstellen: \enquote{zeichen, token, typen, saetze}

Berechnung klassischer linguistischer Kennzahlen.



```{targets tar.enhance.lingstats.normen}
tar_target(var_lingstats.normen,
                f.lingstats(dt.normen,
                            multicore = config$parallel$lingsummarize,
                            cores = fullCores,
                            germanvars = TRUE))
```



```{targets tar.enhance.lingstats.rechtsakte}
tar_target(var_lingstats.rechtsakte,
                f.lingstats(dt.rechtsakte,
                            multicore = config$parallel$lingsummarize,
                            cores = fullCores,
                            germanvars = TRUE))
```




### Finale Datensätze erstellen


```{targets tar.enhance.finalize.normen}
tar_target(dt.normen.final,
           f.finalize_einzelnormen(dt.normen = dt.normen,
                                   lingstats = var_lingstats.normen))
```


```{targets tar.enhance.finalize.rechtsakte}
tar_target(dt.rechtsakte.final,
           f.finalize_rechtsakte(dt.rechtsakte = dt.rechtsakte,
                                 lingstats = var_lingstats.rechtsakte))
```


### Varianten erstellen: Nur Metadaten


```{targets tar.enhance.finalize.normen.meta}
tar_target(dt.normen.meta,
           dt.normen.final[, !"text"])
```



```{targets tar.enhance.finalize.rechtsakte.meta}
tar_target(dt.rechtsakte.meta,
           dt.rechtsakte.final[, !"text"])
```





## Write Targets

Dieser Abschnitt der Pipeline schreibt den Datensatz und alle Hash-Prüfsummen auf die Festplatte.


```{targets tar.write}

values <- tibble::tibble(
                      name = c("download",
                               "conctable",
                               "normen",
                               "normen_meta",
                               "rechtsakte",
                               "rechtsakte_meta",
                               "xml_meta"),
                      input = c(quote(dt.download),
                                quote(dt.conctable),
                                quote(dt.normen.final),
                                quote(dt.normen.meta),
                                quote(dt.rechtsakte.final),
                                quote(dt.rechtsakte.meta),
                                quote(dt.meta)
                                ),
                      filename = paste0(prefix.files,
                                        c("_02_Download-Tabelle",
                                          "_DE_Alle-Rechtsakte-Verzeichnis",
                                          "_DE_CSV_Einzelnormen_Datensatz",
                                          "_DE_CSV_Einzelnormen_Metadaten",
                                          "_DE_CSV_Rechtsakte_Datensatz",
                                          "_DE_CSV_Rechtsakte_Metadaten",
                                          "_DE_CSV_Metadaten-XML_Datensatz"),
                                        ".csv"),
                      dir = c(dir.analysis,
                              rep("output", 6))
                  )




csv.all <- tarchetypes::tar_map(unlist = FALSE,
                                values = values,
                                names = name,
                                tar_target(csv,
                                           f.tar_fwrite(x = input,
                                                        filename = file.path(dir,
                                                                             filename)),
                                           format = "file")
                                )

```








## Report Targets

Dieser Abschnitt der Pipeline erstellt die finalen Berichte (Codebook und Quality Assurance).



### LaTeX-Definitionen schreiben

Um Variablen aus der Pipeline in die LaTeX-Kompilierung einzuführen, müssen diese als .tex-Datei auf die Festplatte geschrieben werden.

```{targets tar.report.defs}
tar_target(latexdefs,
                f.latexdefs(config,
                            dir = "temp",
                            version = datestamp),
	       format = "file")

```

### Zusammenfassungen linguistischer Kennwerte berechnen

```{targets tar.report.lingstatsummary.normen}
tar_target(lingstats.summary.normen,
                f.lingstats_summary(dt.normen.final,
                                    germanvars = TRUE))

```

```{targets tar.report.lingstatsummary.rechtsakte}
tar_target(lingstats.summary.rechtsakte,
                f.lingstats_summary(dt.rechtsakte.final,
                                    germanvars = TRUE))

```





### Report erstellen: Quality Assurance

```{targets tar.report.quality}
tarchetypes::tar_render(report.quality,
                        file.path("reports",
                                  "quality.Rmd"),
                        output_file = file.path("../output",
                                                paste0(config$project$shortname,
                                                       "_",
                                                       datestamp,
                                                       "_QualityAssurance.pdf")))

```



### Report erstellen: Codebook



```{targets tar.report.codebook}
tarchetypes::tar_render(report.codebook,
                        file.path("reports",
                                  "codebook.Rmd"),
                        output_file = file.path("../output",
                                                paste0(config$project$shortname,
                                                       "_",
                                                       datestamp,
                                                       "_Codebook.pdf")))

```






## ZIP Targets

Diese Abschnitt der Pipeline erstellt ZIP-Archive für alle zentralen Rechenergebnisse und speichert diese im Ordner \enquote{output}.


### ZIP erstellen: Static Branching


```{targets tar.zip}

values <- tibble::tibble(
                      name = c("source",
                               "networks",
                               "pdf",
							   "txt",
                               "epub",
                               "xml",
                               "attachments",
                               "einzelnormen",
                               "einzelnormen_meta",
                               "rechtsakte",
                               "rechtsakte_meta",
                               "xml_meta"),
                      input = c(quote(files.source),
                                quote(files.network),
                                quote(files.pdf),
								quote(files.txt),
                                quote(files.epub),
                                quote(files.xml),
                                quote(setdiff(files.xml.all, files.xml)),
                                quote(csv_normen),
                                quote(csv_normen_meta),
                                quote(csv_rechtsakte),
                                quote(csv_rechtsakte_meta),
                                quote(csv_xml_meta)),
                      filename = paste0(prefix.files,
                                        c("_Source_Code",
                                          "_DE_Netzwerke",
                                          "_DE_PDF_Datensatz",
										  "_DE_TXT_Datensatz",
                                          "_DE_EPUB_Datensatz",
                                          "_DE_XML_Datensatz",
                                          "_DE_XML_Anlagen",
                                          "_DE_CSV_Einzelnormen_Datensatz",
                                          "_DE_CSV_Einzelnormen_Metadaten",
                                          "_DE_CSV_Rechtsakte_Datensatz",
                                          "_DE_CSV_Rechtsakte_Metadaten",
                                          "_DE_CSV_Metadaten-XML"),
                                        ".zip"),
                      mode = c(rep("mirror", 2),
                               rep("cherry-pick", 10))
                  )




zip.list <- tarchetypes::tar_map(unlist = FALSE,
                                values = values,
                                names = name,
                                tar_target(zip,
                                           f.tar_zip(x = input,
                                                     filename = filename,
                                                     dir = "output",
                                                     mode = mode),
                                           format = "file")
                                )



```



### ZIP erstellen: Analyse-Dateien

```{targets tar.zip.analysis}
tar_target(zip_analysis,
           f.tar_zip("analysis/",
                     filename = paste(prefix.files,
                                      "DE_Analyse.zip",
                                      sep = "_"),
                     dir = "output",
                     mode = "cherry-pick",
                     report.codebook,    # manually enforced dependency relationship
                     report.quality), # manually enforced dependency relationship
           format = "file")
```





## Kryptographische Hashes

Zum Ende hin werden für alle wichtigen Ergebnisse kryptographische Prüfsummen berechnet, die abschließend (außerhalb der Pipeline) mit dem persönlichen GPG-Key von Seán Fobbe signiert werden.


### Zu hashende ZIP-Archive definieren


```{targets tar.hashes.list}
tar_target(zip.all,
           c(zip_source,
             zip_analysis,
             zip_networks,
             zip_pdf,
			 zip_txt,
             zip_epub,
             zip_xml,
             zip_attachments,
             zip_einzelnormen,
             zip_einzelnormen_meta,
             zip_rechtsakte,
             zip_rechtsakte_meta,
             zip_xml_meta))
```



### Kryptographische Hashes berechnen


```{targets tar.hashes.compute}
tar_target(hashes,
           f.tar_multihashes(c(zip.all,
                               report.codebook[1],
                               report.quality[1]),
                             multicore = config$parallel$multihashes,
                             cores = fullCores))
```



### CSV schreiben: Kryptographische Hashes


```{targets tar.hashes.csv}
tar_target(csv.hashes,
           f.tar_fwrite(x = hashes,
                        filename = file.path("output",
                                             paste0(prefix.files,
                                                    "_KryptographischeHashes.csv"))
                        )
           )
```







# Pipeline: Kompilierung



## Durchführen der Kompilierung


```{r pipeline-run, results = "hide"}
tar_make()
```


## Pipeline archivieren


```{r pipeline-zip}

zip(paste0("output/",
           paste0(config$project$shortname,
                  "_",
                  datestamp),
           "_Targets_Storage.zip"),
    "_targets/")

```




## Visualisierung

```{r, pipeline-graph, fig.width = 10, fig.height = 14}

edgelist <- tar_network(targets_only = TRUE)$edges
setDT(edgelist)

g  <- igraph::graph_from_data_frame(edgelist,
                                    directed = TRUE)


ggraph(g,
       'sugiyama') +
    geom_edge_diagonal(colour = "grey70")+
    geom_node_point(size = 2)+
    geom_node_text(aes(label = name),
                   size = 2,
                   repel = TRUE)+
    theme_void()

```
                       



# Pipeline: Analyse


## Gesamte Liste

Die vollständige Liste aller Targets, inklusive ihres Types und ihrer Größe. Targets die auf Dateien verweisen (z.B. alle PDF-Dateien) geben die Gesamtgröße der Dateien auf der Festplatte an.





```{r, pipeline-list}

meta <- tar_meta(fields = c("type", "bytes", "format"), complete_only = TRUE)
setDT(meta)
meta$MB <- round(meta$bytes / 1e6, digits = 2)

# Gesamter Speicherplatzverbrauch
sum(meta$MB, na.rm = TRUE)

kable(meta[order(type, name)],
      format = "latex",
      align = "r",
      booktabs = TRUE,
      longtable = TRUE) %>% kable_styling(latex_options = "repeat_header")


```

\newpage
## Timing

### Gesamte Laufzeit

```{r, pipeline-runtime}
meta <- tar_meta(fields = c("time", "seconds"), complete_only = TRUE)
setDT(meta)
meta$mins <- round(meta$seconds / 60, digits = 2)

runtime.sum <- sum(meta$seconds)

## Sekunden
print(runtime.sum)

## Minuten
runtime.sum / 60

## Stunden
runtime.sum / 3600
```

### Laufzeit einzelner Targets

Der Zeitpunkt an dem die Targets berechnet wurden und ihre jeweilige Laufzeit in Sekunden.


```{r, pipeline-timing}
kable(meta[order(-seconds)],
      format = "latex",
      align = "r",
      booktabs = TRUE,
      longtable = TRUE) %>% kable_styling(latex_options = "repeat_header")


```




# Warnungen



```{r, pipline-warnings, results = 'asis'}

meta <- tar_meta(fields = "warnings", complete_only = TRUE)
setDT(meta)
meta <- meta[name != "files.network"]
meta$warnings <- gsub("(\\.pdf|\\.html?|\\.txt)|\\.xml", "\\1 \n\n", meta$warnings)

if (meta[,.N > 0]){

    for(i in 1:meta[,.N]){

        cat(paste("##", meta[i]$name), "\n\n")
        cat(paste(meta[i]$warnings, "\n\n"))
        
    }

}else{

    cat("No warnings to report.")

}

```





\newpage
# Fehlermeldungen

```{r, pipeline-errors}

meta <- tar_meta(fields = "error", complete_only = TRUE)
setDT(meta)

if (meta[,.N > 0]){

    for(i in 1:meta[,.N]){

        cat(paste("##", meta[i]$name), "\n\n")
        cat(paste(meta[i]$error, "\n\n"))
        
    }

}else{

    cat("No errors to report.")

}


```



# Dateigrößen der Endergebnisse


## ZIP-Dateien

```{r filesize.zip}

files <- list.files("output", pattern = "\\.zip", full.names = TRUE)

filesize <- round(file.size(files) / 10^6, digits = 2)

table.size <- data.table(basename(files),
                         filesize)


kable(table.size,
      format = "latex",
      align = c("l", "r"),
      format.args = list(big.mark = ","),
      booktabs = TRUE,
      longtable = TRUE,
      col.names = c("Datei",
                    "Größe in MB"))

```

\newpage
## CSV-Dateien

```{r filesize.csv}

files <- list.files("output", pattern = "\\.csv", full.names = TRUE)

filesize <- round(file.size(files) / 10^6, digits = 2)

table.size <- data.table(basename(files),
                         filesize)


kable(table.size,
      format = "latex",
      align = c("l", "r"),
      format.args = list(big.mark = ","),
      booktabs = TRUE,
      longtable = TRUE,
      col.names = c("Datei",
                    "Größe in MB"))

```






# Kryptographische Signaturen

## Signaturen laden

```{r}
tar_load(hashes)
```


## Leerzeichen hinzufügen um bei SHA3-512 Zeilenumbruch zu ermöglichen

Hierbei handelt es sich lediglich um eine optische Notwendigkeit. Die normale 128 Zeichen lange Zeichenfolge von SHA3-512-Signaturen wird ansonsten nicht umgebrochen und verschwindet über die Seitengrenze. Das Leerzeichen erlaubt den automatischen Zeilenumbruch und damit einen für Menschen sinnvoll lesbaren Abdruck im Codebook. Diese Variante wird nur zur Anzeige verwendet und danach verworfen.

```{r}
hashes$sha3.512 <- paste(substr(hashes$sha3.512, 1, 64),
                              substr(hashes$sha3.512, 65, 128))
```



## In Bericht anzeigen

```{r}

kable(hashes[,.(index,filename)],
      format = "latex",
      align = c("p{1cm}",
                "p{13cm}"),
      booktabs = TRUE,
      longtable = TRUE)


kable(hashes[,.(index,sha2.256)],
      format = "latex",
      align = c("c",
                "p{13cm}"),
      booktabs = TRUE,
      longtable = TRUE)


```

\newpage

```{r}

kable(hashes[,.(index,sha3.512)],
      format = "latex",
      align = c("c",
                "p{13cm}"),
      booktabs = TRUE,
      longtable = TRUE)
```








\newpage

```{r, results = "asis", echo = FALSE}
cat(readLines("CHANGELOG.md"),
    sep = "\n")

```


# Abschluss

```{r}

## Datumsstempel
print(datestamp) 

## Datum und Uhrzeit (Anfang)
print(begin.script)


## Datum und Uhrzeit (Ende)
end.script <- Sys.time()
print(end.script)


## Laufzeit des gesamten Skriptes
print(end.script - begin.script)

```


# Parameter für strenge Replikationen


```{r}
system2("openssl", "version", stdout = TRUE)

sessionInfo()

```


# Literaturverzeichnis
