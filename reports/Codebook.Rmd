---
title: "Codebook | Corpus des Deutschen Bundesrechts (C-DBR)"
author: Seán Fobbe
geometry: margin=3cm
papersize: a4
fontsize: 11pt
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
    pandoc_args: --listings
    includes:
      in_header: ../tex/Preamble_DE.tex
      before_body: [../temp/Definitions.tex, ../tex/Titlepage_Codebook.tex]
bibliography: ../temp/packages.bib
nocite: '@*'
---


```{r, setup, include=FALSE}
knitr::opts_chunk$set(fig.path = file.path("..", "analysis/"),
                      dev = config$fig$format,
                      dpi = config$fig$dpi,
                      fig.align = config$fig$align,
                      echo = FALSE,
                      warning = FALSE,
                      message = FALSE)

```


```{r, echo = FALSE}
tar_load(latexdefs)
tar_load(lingstats.summary.normen)
tar_load(lingstats.summary.rechtsakte)
tar_load(dt.normen.final)
tar_load(dt.rechtsakte.final)
tar_load(dt.meta)
tar_load(files.xml)
tar_load(files.pdf)
tar_load(files.txt)
tar_load(files.epub)
tar_load(dt.var_codebook)
```

```{r, echo = FALSE}

## Frequenztabellen

varlist  <-  c("periodikum",
               "ausfertigung_jahr")


freqtable.normen <- f.fast.freqtable(dt.normen.final,
                                     varlist = varlist,
                                     sumrow = TRUE,
                                     output.list = TRUE,
                                     output.kable = FALSE,
                                     output.csv = FALSE)

freqtable.rechtsakte <- f.fast.freqtable(dt.rechtsakte.final,
                                         varlist = varlist,
                                         sumrow = TRUE,
                                         output.list = TRUE,
                                         output.kable = FALSE,
                                         output.csv = FALSE)

freqtable.xmlmeta <- f.fast.freqtable(dt.meta,
                                      varlist = varlist,
                                      sumrow = TRUE,
                                      output.list = TRUE,
                                      output.kable = FALSE,
                                      output.csv = FALSE)

```


# Einführung

Dem **Bundesrecht** kommt im Normengefüge der Bundesrepublik Deutschland herausragende Bedeutung zu. Zwar sind die Länder gemäß Art. 30, 70 GG primär für die Gesetzgebung zuständig, im Katalog der Art. 71 ff GG sind aber derart viele Kompetenzen dem Bund zugewiesen, dass das Bundesrecht praktisch jedes rechtliche Problem in der Bundesrepublik dominiert. Ausnahmen sind in der Regel nur die Bereiche innere Sicherheit, Bildung und Kultur, die weitgehend in der Hand der Bundesländer verblieben sind. Aber auch in diesen Bereichen finden sich Regelungen des Bundes. Beispiele dafür sind manche Regelungen des Bundespolizeigesetzes (BPolG) oder das Kulturgutschutzgesetz (KGSG).

Bundesgesetze werden vom Bundestag im Zusammenwirken mit dem Bundesrat erlassen und vom Bundespräsidenten ausgefertigt (Art. 76 ff GG). Das Initiativrecht liegt bei Abgeordneten aus der Mitte des Bundestags, der Bundesregierung und dem Bundesrat (Art. 76 Abs. 1 GG). Der Bundesrat ist je nach Gesetzescharakter mit einem Zustimmungserfordernis oder einem Einspruchsrecht beteiligt (Art. 77, 78 GG).

Verordnungen werden in der Regel von der Exekutive erlassen, in seltenen Fällen vom Bundestag selbst. Durch Bundesgesetz können nur Bundesregierung, Bundesminister oder Landesregierungen hierzu ermächtigt werden (Art. 80 Abs. 1 S.1 GG), eine im Gesetz vorgesehen Sub-Delegation ist aber möglich (Art. 80 Abs. 1 S. 4 GG). Verordnungen müssen einem speziellen Bestimmtheitsgebot genügen und ihre Rechtsgrundlage in der Verordnung angeben (Art. 80 Abs. 1 S. 2 und 3). Der Erlass von Verordnungen erfordert zudem nicht selten die Zustimmung des Bundesrates, entweder aufgrund von Art. 80 Abs. 2 GG oder bedingt durch eine Regelung in einem einfachen Bundesgesetz.
 
Warum dieser Datensatz? Die quantitative Analyse von juristischen Texten, insbesondere von Gesetzen und Verordnungen, ist in den deutschen Rechtswissenschaften ein noch junges und kaum bearbeitetes Feld.\footnote{Positive Ausnahmen finden sich vor allem unter: \url{https://www.quantitative-rechtswissenschaft.de/}} Zu einem nicht unerheblichen Teil liegt dies auch daran, dass die Anzahl an frei nutzbaren Datensätzen außerordentlich gering ist.
 
Die meisten hochwertigen Datensätze lagern (fast) unerreichbar in kommerziellen Datenbanken und sind wissenschaftlich gar nicht oder nur gegen Entgelt zu nutzen. Frei verfügbare Datenbanken wie \emph{Opinio Iuris}\footnote{\url{https://opinioiuris.de/}} und \emph{openJur}\footnote{\url{https://openjur.de/}} verbieten ausdrücklich das maschinelle Auslesen der Rohdaten.\footnote{Openjur beabsichtigt eine API anzubieten, diese war aber im Januar 2021 immernoch nicht verfügbar. Openjur ist seit 2008 in Betrieb.} Wissenschaftliche Initiativen wie der Juristische Referenzkorpus (JuReKo) sind nach jahrelanger Arbeit hinter verschlossenen Türen verschwunden.
 
In einem funktionierenden Rechtsstaat muss die Rechtsetzung öffentlich, transparent und nachvollziehbar sein. Im 21. Jahrhundert bedeutet dies auch, dass sie quantitativen Analysen zugänglich sein muss. Der Erstellung und Aufbereitung des Datensatzes liegen daher die Prinzipien der allgemeinen Verfügbarkeit durch Urheberrechtsfreiheit, strenge Transparenz und vollständige wissenschaftliche Reproduzierbarkeit zugrunde. Die FAIR-Prinzipien (Findable, Accessible, Interoperable and Reusable) für freie wissenschaftliche Daten inspirieren sowohl die Konstruktion, als auch die Art der Publikation.\footnote{Wilkinson, M., Dumontier, M., Aalbersberg, I. et al. The FAIR Guiding Principles for Scientific Data Management and Stewardship. Sci Data 3, 160018 (2016). \url{https://doi.org/10.1038/sdata.2016.18}}







# Nutzung

Die Daten sind in offenen, interoperablen und weit verbreiteten Formaten (CSV, XML, TXT, PDF, EPUB) veröffentlicht. Sie lassen sich grundsätzlich mit allen modernen Programmiersprachen (z.B. Python oder R), sowie mit grafischen Programmen nutzen.

 **Wichtig:** Nicht vorhandene Werte sind sowohl in den Dateinamen als auch in der CSV-Datei mit \enquote{NA} codiert.


## CSV-Dateien

Am einfachsten ist es die **CSV-Dateien** einzulesen. Die Nutzung der CSV-Varianten ist aus Qualitätsgründen und den umfangreicheren Metadaten \emph{empfohlen}. CSV\footnote{Das CSV-Format ist in RFC 4180 definiert, siehe \url{https://tools.ietf.org/html/rfc4180}} ist ein einfaches und maschinell gut lesbares Tabellen-Format. In diesem Datensatz sind die Werte komma-separiert. Jede Spalte entspricht einer Variable, jede Zeile einer Einzelnorm bzw. einem Rechtsakt (je nach Variante). Die Variablen sind unter Punkt \ref{variablen} genauer erläutert.

Hier empfehle ich für **R** das package **data.table** (via CRAN verfügbar). Dessen Funktion **fread()** ist etwa zehnmal so schnell wie die normale **read.csv()**-Funktion in Base-R. Sie erkennt auch den Datentyp von Variablen sicherer. Ein Beispiel:


```{r eval = FALSE, echo = TRUE}
library(data.table)
csv.dbr <- fread("filename.csv")

```


## XML-Dateien

Das Einlesen der **XML-Rohdaten** ist komplex und die Entscheidung welche XML-Nodes zu extrahieren sind wird ganz erheblich von der Forschungsfrage beeinflusst. Falls Sie über vertiefte XML-Kenntnisse verfügen, sollten Sie eine eigenständige Extraktion dennoch in Erwägung ziehen, weil sie so die Datenanalyse besser auf Ihre Bedürfnisse anpassen können. Lesen Sie hierfür bitte die Document Type Definition (DTD) genau und greifen Sie ggf. auf den im Source Code zur Verfügung gestellten XML Parser zurück.




## TXT-Dateien

Die TXT-Dateien enthalten nur sehr rudimentäre Metadaten! Benutzen Sie daher für statistische Analysen vorzugsweise die CSV- oder XML-Dateien. Die **TXT-Dateien** inklusive Metadaten können zum Beispiel mit **R** und dem package **readtext** (via CRAN verfügbar) eingelesen werden. Ein Vorschlag:

```{r eval = FALSE, echo = TRUE}
library(readtext)
txt.dbr <- readtext("./*.txt",
                    docvarsfrom = "filenames", 
                    docvarnames = c("kurztitel",
                                    "langtitel"),
                    dvsep = "_", 
                    encoding = "UTF-8")
```






# Konstruktion



## Beschreibung des Datensatzes

Der Datensatz ist eine möglichst vollständige Sammlung der konsolidierten Fassungen aller Gesetze und Verordnungen auf Bundesebene. Änderungsgesetze und -verordnungen sind nicht enthalten. Er enthält alle Rechtsakte, die auf der amtlichen Webseite \enquote{Gesetze im Internet} des Bundesministerium des Justiz am jeweiligen Stichtag verfügbar waren. Die Stichtage für jede Version sind in der Versionsnummer festgehalten.

Zusätzlich zu den einfach maschinenlesbaren Formaten (CSV und TXT) sind die XML-, PDF- und EPUB-Rohdaten enthalten, damit Analysten gegebenenfalls ihre eigene Konvertierung vornehmen können. Die Rohdaten wurden inhaltlich nicht verändert. Die PDF- und EPUB-Varianten der Rechtsakte sollen primär traditionelle juristische Forschung und \emph{mixed methods}-Ansätze unterstützen.

 In diesem Datensatz sind nur Rechtsakte mit Außenwirkung (d.h. das Grundgesetz, Bundesgesetze und Bundesverordnungen) enthalten. Verwaltungsvorschriften sind nicht Teil des Datensatzes.




## Datenquellen

\begin{centering}
\begin{longtable}{P{5cm}p{9cm}}

\toprule

 Datenquelle & Fundstelle \\

\midrule

 Primäre Datenquelle & \url{https://www.gesetze-im-internet.de/}\\
 Source Code & \url{\softwareversionurldoi}\\

\bottomrule

\end{longtable}
\end{centering}




## Sammlung der Daten

Die Daten wurden vollautomatisiert gesammelt und mit Abschluss der Verarbeitung kryptographisch signiert. Die Webseite des Justizministeriums ist laut dem Reiter \enquote{Hinweise}\footnote{\url{https://www.gesetze-im-internet.de/hinweise.html}} ausdrücklich für die vollautomatisierte Datensammlung freigegeben. Der Abruf geschieht ausschließlich über TLS-verschlüsselte Verbindungen.



```{r, C-DBR_00_Pipeline_Graph, fig.width = 12, fig.height = 14, fig.pos = "p", fig.cap = "Der vollständige Prozess der Datensatz-Kompilierung."}


edgelist <- tar_network(targets_only = TRUE)$edges
setDT(edgelist)

g  <- igraph::graph.data.frame(edgelist,
                               directed = TRUE)


ggraph(g,
       'sugiyama') + 
    geom_edge_diagonal(colour = "grey70")+
    geom_node_point(size = 2)+
    geom_node_text(aes(label = name),
                   size = 2,
                   repel = TRUE)+
    theme_void()+
    labs(
        title = paste(prefix.figuretitle,
                      "| Vollständiger Prozess der Datensatz-Kompilierung"),
        caption = caption
    )+
    theme(
        plot.title = element_text(size = 14,
                                  face = "bold"),
        plot.margin = margin(10, 20, 10, 10)
    )

```




## Source Code und Compilation Report

Der gesamte Source Code --- sowohl für die Erstellung des Datensatzes, als auch für dieses Codebook --- ist öffentlich einsehbar und dauerhaft erreichbar im wissenschaftlichen Archiv des CERN unter dieser Addresse hinterlegt: \softwareversionurldoi

Mit jeder Kompilierung des vollständigen Datensatzes wird auch ein umfangreicher **Compilation Report** in einem attraktiv designten PDF-Format erstellt (ähnlich diesem Codebook). Der Compilation Report enthält den vollständigen Source Code, dokumentiert relevante Rechenergebnisse, gibt sekundengenaue Zeitstempel an und ist mit einem klickbaren Inhaltsverzeichnis versehen. Er ist zusammen mit dem Source Code hinterlegt. Wenn Sie sich für Details des Erstellungs-Prozesses interessieren, lesen Sie diesen bitte zuerst.




## Grenzen des Datensatzes

Nutzer sollten folgende wichtige Grenzen beachten:
 

- Der Datensatz enthält nur das, was das Bundesjustizministerium auch tatsächlich veröffentlicht (\emph{publication bias}). Es fehlen insbesondere Änderungsgesetze und -verordnungen. Manche Rechtsakte sind zudem nur mit den Metadaten nachgewiesen --- ihr Inhalt fehlt aus technischen Gründen oder weil in der Bereinigten Sammlung Bundesgesetzblatt Teil III nur bibliographische Angaben enthalten sind (§ 3 Abs. 2 BRSG).\footnote{\url{https://www.gesetze-im-internet.de/hinweise.html}}
- Es kann aufgrund technischer Grenzen bzw. Fehler sein, dass manche --- im Grunde verfügbare --- Rechtsakte nicht oder nicht korrekt abgerufen werden (\emph{automation bias}).
- Es sind nur am Tag des Abrufs veröffentlichte konsolidierte Rechtsakte enthalten, eine diachronische Untersuchung muss somit mehrere verfügbare Versionen auswerten (\emph{temporal bias}). 




## Urheberrechtsfreiheit von Rohdaten und Datensatz 

An den Rechtsakten und Rechtsnormen besteht gem. § 5 Abs. 1 UrhG kein Urheberrecht, da sie amtliche Werke sind. § 5 UrhG ist auf amtliche Datenbanken analog anzuwenden (BGH, Beschluss vom 28.09.2006, I ZR 261/03, \enquote{Sächsischer Ausschreibungsdienst}).

 Alle eigenen Beiträge (z.B. durch Zusammenstellung und Anpassung der Metadaten) und damit den gesamten Datensatz stelle ich gemäß einer \emph{CC0 1.0 Universal Public Domain Lizenz} vollständig urheberrechtsfrei.




## Metadaten

Alle Metadaten wurden aus den XML-Rohdaten zeitgleich mit dem Text der Normen extrahiert. Der volle Satz an Metadaten ist nur in den CSV-Dateien enthalten. Bitte beachten Sie, dass bei weitem nicht alle XML-Nodes ausgewertet wurden. Viele Nodes enthalten nur optische Informationen und wurden deshalb ignoriert. Manche Nodes (z.B. einzelne Absätze, Listen) wurden nicht extrahiert, weil nicht alle Normen in Absätze und Listen unterteilt sind und die Bereitstellung in einem nicht-hierarchischen Format wie CSV keine Vorteile gegenüber dem XML-Format bringen würde.
 
Die Dateinamen der PDF-, TXT und EPUB-Dateien enthalten nur eine Abkürzung und einen modifizierten Langtitel (auf 200 Zeichen gekürzt und um Sonderzeichen bereinigt). Diese wurden aus den jeweiligen Header-Markierungen der HTML-Seiten extrahiert.



### Schema für die Dateinamen (PDF, TXT, EPUB)

\begin{verbatim}
[Abkürzung]_[modifizierter_Langtitel]
\end{verbatim}


### Beispiel eines Dateinamens

\begin{verbatim}
2.WasSV_ZweiteWassersicherstellungsverordnung.pdf
\end{verbatim}


## Qualitätsprüfung

Insgesamt werden zusammen mit jeder Kompilierung Dutzende Tests zur Qualitätsprüfung durchgeführt. Alle Ergebnisse der Qualitätsprüfungen sind aggregiert im Compilation Report und einzeln im Archiv \enquote{analyse} zusammen mit dem Datensatz veröffentlicht.







# Varianten und Zielgruppen

Dieser Datensatz ist in verschiedenen Varianten verfügbar, die sich an unterschiedliche Zielgruppen richten. Zielgruppe sind nicht nur quantitativ forschende RechtswissenschaftlerInnen, sondern auch traditionell arbeitende JuristInnen. Idealerweise müssen quantitative Methoden ohnehin immer durch qualitative Interpretation, Bildung von Theorien und kritische Auseinandersetzung verstärkt werden (\emph{mixed methods approach}).

Lehrende werden zudem von den vorbereiteten Tabellen und Diagrammen besonders profitieren, die bei der Erläuterung der Charakteristika der Daten hilfreich sein können und Zeit im universitären Alltag sparen. Alle Tabellen und Diagramme liegen auch als separate Dateien vor um sie einfach z.B. in Präsentations-Folien oder Handreichungen zu integrieren.

\begin{centering}
\begin{longtable}{P{3.5cm}p{10.5cm}}

\toprule

Variante & Zielgruppe und Beschreibung\\

\midrule
\endhead

 CSV\_Einzelnormen\_ Datensatz & \textbf{Legal Tech/Quantitative Forschung}. Diese CSV-Datei ist eine der für statistische Analysen empfohlenen Varianten des Datensatzes. Sie enthält den Volltext aller Rechtsakte, disaggregiert nach Einzelnormen, sowie alle in diesem Codebook beschriebenen Metadaten. Enthält nur Rechtsakte, für die mindestens eine Einzelnorm mit Normtext veröffentlicht wurde!\\
 CSV\_Einzelnormen\_ Metadaten & \textbf{Legal Tech/Quantitative Forschung}. Wie die andere CSV-Datei mit Einzelnormen, nur ohne die Normtexte. Sinnvoll für Analyst:innen, die sich nur für die Metadaten interessieren und Speicherplatz sparen wollen. Enthält nur Rechtsakte, für die mindestens eine Einzelnorm mit Normtext veröffentlicht wurde!\\
 CSV\_Rechtsakte\_ Datensatz & \textbf{Legal Tech/Quantitative Forschung}. Diese CSV-Datei ist eine der für statistische Analysen empfohlenen Varianten des Datensatzes. Sie enthält den Volltext aller Rechtsakte, sowie fast alle in diesem Codebook beschriebenen Metadaten. Die gegenüber den Einzelnormen fehlenden Metadaten betreffen vor allem Gliederungsdaten (z.B. Gliederungsüberschrift), die auf Rechtsakts-Ebene keinen Sinn ergeben. Wurde durch ein Zusammenfügen der Einzelnorm-Variante erstellt. Enthält nur Rechtsakte, für die mindestens eine Einzelnorm mit Normtext veröffentlicht wurde!\\
 CSV\_Rechtsakte\_ Metadaten & \textbf{Legal Tech/Quantitative Forschung}. Wie die andere CSV-Datei mit Rechtsakten, nur ohne die Normtexte. Sinnvoll für Analyst:innen, die sich nur für die Metadaten interessieren und Speicherplatz sparen wollen. Enthält nur Rechtsakte, für die mindestens eine Einzelnorm mit Normtext veröffentlicht wurde!\\
 CSV\_MetadatenXML & \textbf{Legal Tech/Quantitative Forschung}. Diese CSV-Datei enthält Metadaten für jeden auf der amtlichen Webeite nachgewiesenen Rechtsakt, unabhängig davon, ob mit oder ohne Normtext veröffentlicht. Die Zahl der Rechtsakte ist daher um etwa 1000 höher als bei den anderen CSV-Dateien, es sind aber keine Normtexte enthalten.\\
 XML\_Datensatz & \textbf{Legal Tech/Quantitative Forschung}. Die XML-Rohdaten. Alle CSV-Dateien wurden aus diesen Rohdaten extrahiert. XML ist ein komplexes Format und daher nur für entsprechend versierte Forscher:innen geeignet.\\
 XML\_Anlagen & \textbf{Legal Tech/Quantitative Forschung}. Manche XML-Dateien verweisen auf Anlagen, vorwiegend Bild-Dateien. Diese sind hier zusammengefasst.\\
 PDF\_Datensatz & \textbf{Traditionelle juristische Forschung}. Die PDF-Dokumente wie sie vom Bundesjustizministerium auf der amtlichen Webseite bereitgestellt werden, jedoch verbessert durch semantisch hochwertige Dateinamen, die sowohl die Abkürzung, als auch einen modifizierten Langtitel enthalten. Die Dateinamen sind so konzipiert, dass sie auch für traditionelle qualitative juristische Arbeit einen erheblichen Mehrwert bieten. Im Vergleich zu den CSV-Dateien enthalten die Dateinamen nur einen drastisch reduzierten Umfang an Metadaten, um Kompatibilitätsprobleme unter Windows zu vermeiden und die Lesbarkeit zu verbessern. Besonders geeignet für die Arbeit an Desktop PCs.\\
 EPUB\_Datensatz & \textbf{Traditionelle juristische Forschung}. Die PDF-Dokumente wie sie vom Bundesjustizministerium auf der amtlichen Webseite bereitgestellt werden, jedoch verbessert durch semantisch hochwertige Dateinamen, die sowohl die Abkürzung, als auch einen modifizierten Langtitel enthalten. Die Dateinamen sind so konzipiert, dass sie auch für traditionelle qualitative juristische Arbeit einen erheblichen Mehrwert bieten. Im Vergleich zu den CSV-Dateien enthalten die Dateinamen nur einen drastisch reduzierten Umfang an Metadaten, um Kompatibilitätsprobleme unter Windows zu vermeiden und die Lesbarkeit zu verbessern. Besonders geeignet für die Arbeit an mobilen Endgeräten, weil sich das Format der Bildschirmgröße anpassen kann.\\
 TXT\_Datensatz & \textbf{Subsidiär}. Diese Variante enthält die vollständigen aus den PDF-Dateien extrahierten Normtexte der Rechtsakte, aber nur einen drastisch reduzierten Umfang an Metadaten, der dem der PDF-Dateien entspricht. Die TXT-Dateien sind optisch an das Layout der PDF-Dateien angelehnt. Geeignet für qualitative Forscher, die nur wenig Speicherplatz oder eine langsame Internetverbindung zur Verfügung haben und für quantitative Forscher, die beim Einlesen der CSV-Dateien Probleme haben.\\
 Netzwerke & \textbf{Experimentell}. Die Gliederungshierarchie aller Rechtsakte wurde in eine Netzwerkstruktur übersetzt und ist in verschiedenen Formaten bereitgestellt (GraphML, Adjazenzmatrizen und Edge Lists). Aus dieser Netzwerkstruktur wurden zudem hierarchische Dendrogramme erstellt um einen visuellen Überblick zu bieten. Einzelnormen sind in den Netzwerkstrukturen aktuell noch nicht berücksichtigt. Diese Variante ist noch hoch-experimentell, sollte also nicht ohne genaue Prüfung für die eigene Forschung verwendet werden.\\ 
 ANALYSE & \textbf{Alle Lehrenden und Forschenden}. Dieses Archiv enthält alle während dem Kompilierungs- und Prüfprozess erstellten Tabellen (CSV) und Diagramme (PDF, PNG) im Original. Sie sind inhaltsgleich mit den in diesem Codebook verwendeten Tabellen und Diagrammen. Das PDF-Format eignet sich besonders für die Verwendung in gedruckten Publikationen, das PNG-Format besonders für die Darstellung im Internet. Analyst:innen mit fortgeschrittenen Kenntnissen in R können auch auf den Source Code zurückgreifen. Empfohlen für Nutzer die einzelne Inhalte aus dem Codebook für andere Zwecke (z.B. eigene Publikationen) weiterverwenden möchten.\\


\bottomrule

\end{longtable}
\end{centering}




\newpage






# Variablen


## Hinweise


- Fehlende Werte sind immer mit \enquote{NA} codiert
- Strings können grundsätzlich alle in UTF-8 definierten Zeichen (insbesondere Buchstaben, Zahlen und Sonderzeichen) enthalten.
- Alle Variablen sind in der hier beschriebenen Form nur in der CSV-Datei enthalten. Die meisten davon sind jedoch aus gleichlautenden oder ähnlich lautenden Nodes in den XML-Daten vorhanden. 



## Erläuterungen der einzelnen Variablen


```{r}
kable(dt.var_codebook,
      format = "latex",
      align = 'P{3.5cm}P{3cm}p{8cm}',
      booktabs = TRUE,
      longtable = TRUE,
      escape = FALSE,
      col.names = c("Variable",
                    "Typ",
                    "Erläuterung"))  %>% kable_styling(latex_options = "repeat_header")
```


\newpage

## Konkordanztabelle: XML-Struktur und CSV-Variablen


\begin{longtable}{lll}

\toprule

 CSV-Variable & XPath & Attribut\\

\midrule

 text & /norm/textdaten/text/Content & -\\
 builddate\_original & /norm & builddate\\
 fundstellentyp & /norm/metadaten/fundstelle & typ\\
 periodikum & /norm/metadaten/fundstelle/periodikum & -\\
 zitstelle & /norm/metadaten/fundstelle/zitstelle & -\\
 stand & /norm/metadaten/standangabe/standtyp & -\\
      & /norm/metadaten/standangabe/standkommentar & -\\
 aufh & /norm/metadaten/standangabe/standtyp & -\\
      & /norm/metadaten/standangabe/standkommentar & -\\
 neuf & /norm/metadaten/standangabe/standtyp & -\\
      & /norm/metadaten/standangabe/standkommentar & -\\
 hinweis & /norm/metadaten/standangabe/standtyp & -\\
       & /norm/metadaten/standangabe/standkommentar & -\\
 sonst & /norm/metadaten/standangabe/standtyp & -\\
       & /norm/metadaten/standangabe/standkommentar & -\\
 check\_* & /norm/metadaten/standangabe & checked\\
 amtabk & /norm/metadaten/amtabk & - \\
 jurabk & /norm/metadaten/jurabk & - \\
 langue & /norm/metadaten/langue & - \\
 kurzue & /norm/metadaten/kurzue & - \\
 gliederungskennzahl & /norm/metadaten/gliederungseinheit/gliederungskennzahl & - \\
 gliederungsbez & /norm/metadaten/gliederungseinheit/gliederungsbez & - \\
 gliederungstitel & /norm/metadaten/gliederungseinheit/gliederungstitel & - \\
 enbez & /norm/metadaten/enbez & - \\
 ausfertigung\_datum & /norm/metadaten/ausfertigung-datum & - \\
\bottomrule

\end{longtable}



\newpage



# Linguistische Kennzahlen

## Erläuterung der Kennzahlen

 Zur besseren Einschätzung des inhaltlichen Umfangs des Korpus dokumentiere ich an dieser Stelle die Verteilung der Werte für drei verschiedene klassische linguistische Kennzahlen:

 \medskip

\begin{centering}
\begin{longtable}{P{3.5cm}p{10.5cm}}

\toprule

Variable & Definition\\

\midrule

 Zeichen & Zeichen entsprechen grob den \emph{Graphemen}, den kleinsten funktionalen Einheiten in einem Schriftsystem. Beispiel: das Wort \enquote{Richterin} besteht aus 9 Zeichen.\\
 Tokens & Eine beliebige Zeichenfolge, getrennt durch whitespace-Zeichen, d.h. ein Token entspricht in der Regel einem \enquote{Wort}, kann aber gelegentlich auch sinnlose Zeichenfolgen enthalten, weil es rein syntaktisch berechnet wird.\\
 Typen & Einzigartige Tokens. Beispiel: wenn das Token \enquote{gewerblich} mehrmals in einer Norm vorhanden ist, wird es als ein Typ gezählt.\\
 Sätze & Entsprechen in etwa dem üblichen Verständnis eines Satzes. Die Regeln für die Bestimmung von Satzanfang und Satzende sind im Detail aber sehr komplex und in \enquote{Unicode Standard: Annex No 29} beschrieben. Für Rechtsnormen ist diese Zählweise vermutlich nicht robust genug, interpretieren Sie die Ergebnisse mit großer Vorsicht!\\

\bottomrule

\end{longtable}
\end{centering}






## Kennzahlen: Einzelnormen


```{r}
setnames(lingstats.summary.normen, c("Variable",
                                     "Gesamt",
                                     "Min",
                                     "1. Quartil",
                                     "Median",
                                     "Mittel",
                                     "3. Quartil",
                                     "Max"))

lingstats.summary.normen$Variable <- c("Zeichen",
                                       "Tokens",
                                       "Typen",
                                       "Sätze")

kable(lingstats.summary.normen,
      digits = 2,
      format.args = list(big.mark = ","),
      format = "latex",
      booktabs = TRUE,
      longtable = TRUE)
```




## Kennzahlen: Rechtsakte


```{r}
setnames(lingstats.summary.rechtsakte, c("Variable",
                                         "Gesamt",
                                         "Min",
                                         "1. Quartil",
                                         "Median",
                                         "Mittel",
                                         "3. Quartil",
                                         "Max"))

lingstats.summary.rechtsakte$Variable <- c("Zeichen",
                                           "Tokens",
                                           "Typen",
                                           "Sätze")

kable(lingstats.summary.rechtsakte,
      digits = 2,
      format.args = list(big.mark = ","),
      format = "latex",
      booktabs = TRUE,
      longtable = TRUE)
```




## Verteilung Zeichen



```{r C-DBR_01_Einzelnormen_Density_Zeichen, fig.height = 6, fig.width = 9}
ggplot(data = dt.normen.final)+
    geom_density(aes(x = zeichen),
                 fill = "black") +
    scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
                  labels = trans_format("log10", math_format(10^.x)))+
    annotation_logticks(sides = "b")+ 
    coord_cartesian(xlim = c(1, 10^6))+
    theme_bw()+
    labs(
        title = paste(prefix.figuretitle,
                      "| Verteilung der Zeichen je Norm"),
        caption = caption,
        x = "Zeichen",
        y = "Dichte"
    )+
    theme(
        text = element_text(size = 14),
        plot.title = element_text(size = 14,
                                  face = "bold"),
        legend.position = "none",
        plot.margin = margin(10, 20, 10, 10)
    )
```



\vspace{1cm}


```{r C-DBR_01_Rechtsakte_Density_Zeichen, fig.height = 6, fig.width = 9}
ggplot(data = dt.rechtsakte.final)+
    geom_density(aes(x = zeichen),
                 fill = "black") +
    scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
                  labels = trans_format("log10", math_format(10^.x)))+
    annotation_logticks(sides = "b")+ 
    coord_cartesian(xlim = c(1, 10^6))+
    theme_bw()+
    labs(
        title = paste(prefix.figuretitle,
                      "| Verteilung der Zeichen je Rechtsakt"),
        caption = caption,
        x = "Zeichen",
        y = "Dichte"
    )+
    theme(
        text = element_text(size = 14),
        plot.title = element_text(size = 14,
                                  face = "bold"),
        legend.position = "none",
        plot.margin = margin(10, 20, 10, 10)
    )
```



## Verteilung Tokens


```{r C-DBR_02_Einzelnormen_Density_Tokens, fig.height = 6, fig.width = 9}
ggplot(data = dt.normen.final)+
    geom_density(aes(x = tokens),
                 fill = "black") +
    scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
                  labels = trans_format("log10", math_format(10^.x)))+
    annotation_logticks(sides = "b")+ 
    coord_cartesian(xlim = c(1, 10^6))+
    theme_bw()+
    labs(
        title = paste(prefix.figuretitle,
                      "| Verteilung der Tokens je Norm"),
        caption = caption,
        x = "Tokens",
        y = "Dichte"
    )+
    theme(
        text = element_text(size = 14),
        plot.title = element_text(size = 14,
                                  face = "bold"),
        legend.position = "none",
        plot.margin = margin(10, 20, 10, 10)
    )


```

\vspace{1cm}



```{r C-DBR_02_Rechtsakte_Density_Tokens, fig.height = 6, fig.width = 9}
ggplot(data = dt.rechtsakte.final)+
    geom_density(aes(x = tokens),
                 fill = "black")+
    scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
                  labels = trans_format("log10", math_format(10^.x)))+
    annotation_logticks(sides = "b")+ 
    coord_cartesian(xlim = c(1, 10^6))+
    theme_bw() +
    labs(
        title = paste(prefix.figuretitle,
                      "| Verteilung der Tokens je Rechtsakt"),
        caption = caption,
        x = "Tokens",
        y = "Dichte"
    )+
    theme(
        text = element_text(size = 14),
        plot.title = element_text(size = 14,
                                  face = "bold"),
        legend.position = "none",
        plot.margin = margin(10, 20, 10, 10)
    )
```




## Verteilung Typen

```{r C-DBR_03_Einzelnormen_Density_Typen, fig.height = 6, fig.width = 9}
ggplot(data = dt.normen.final)+
    geom_density(aes(x = typen),
                 fill = "black")+
    scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
                  labels = trans_format("log10", math_format(10^.x)))+
    annotation_logticks(sides = "b")+ 
    coord_cartesian(xlim = c(1, 10^6))+
    theme_bw()+
    labs(
        title = paste(prefix.figuretitle,
                      "| Verteilung der Typen je Norm"),
        caption = caption,
        x = "Typen",
        y = "Dichte"
    )+
    theme(
        text = element_text(size = 14),
        plot.title = element_text(size = 14,
                                  face = "bold"),
        legend.position = "none",
        plot.margin = margin(10, 20, 10, 10)
    )
```


\vspace{1cm}


```{r C-DBR_03_Rechtsakte_Density_Typen, fig.height = 6, fig.width = 9}
ggplot(data = dt.rechtsakte.final)+
    geom_density(aes(x = typen),
                 fill = "black")+
    scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
                  labels = trans_format("log10", math_format(10^.x)))+
    annotation_logticks(sides = "b")+ 
    coord_cartesian(xlim = c(1, 10^6))+
    theme_bw()+
    labs(
        title = paste(prefix.figuretitle,
                      "| Verteilung der Typen je Rechtsakt"),
        caption = caption,
        x = "Typen",
        y = "Dichte"
    )+
    theme(
        text = element_text(size = 14),
        plot.title = element_text(size = 14,
                                  face = "bold"),
        legend.position = "none",
        plot.margin = margin(10, 20, 10, 10)
    )
```




## Verteilung Sätze




```{r C-DBR_04_Einzelnormen_Density_Saetze, fig.height = 6, fig.width = 9}
ggplot(data = dt.normen.final)+
    geom_density(aes(x = saetze),
                 fill = "black")+
    scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
                  labels = trans_format("log10", math_format(10^.x)))+
    annotation_logticks(sides = "b")+ 
    coord_cartesian(xlim = c(1, 10^6))+
    theme_bw()+
    labs(
        title = paste(prefix.figuretitle,
                      "| Verteilung der Sätze je Norm"),
        caption = caption,
        x = "Sätze",
        y = "Dichte"
    )+
    theme(
        text = element_text(size = 14),
        plot.title = element_text(size = 14,
                                  face = "bold"),
        legend.position = "none",
        plot.margin = margin(10, 20, 10, 10)
    )


```

\vspace{1cm}




```{r C-DBR_04_Rechtsakte_Density_Saetze, fig.height = 6, fig.width = 9}
ggplot(data = dt.rechtsakte.final)+
    geom_density(aes(x = saetze),
                 fill = "black")+
    scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
                  labels = trans_format("log10", math_format(10^.x)))+
    annotation_logticks(sides = "b")+ 
    coord_cartesian(xlim = c(1, 10^6))+ 
    theme_bw()+
    labs(
        title = paste(prefix.figuretitle,
                      "| Verteilung der Sätze je Rechtsakt"),
        caption = caption,
        x = "Sätze",
        y = "Dichte"
    )+
    theme(
        text = element_text(size = 14),
        plot.title = element_text(size = 14,
                                  face = "bold"),
        legend.position = "none",
        plot.margin = margin(10, 20, 10, 10)
    )

```



\newpage
\ra{1.4}

# Inhalt


## Nach Periodikum


### Einzelnormen



```{r C-DBR_05_Einzelnormen_Barplot_Periodikum, fig.height = 10, fig.width = 8}
ggplot(data = freqtable.normen$periodikum[-.N])+
    geom_bar(aes(x = reorder(periodikum,
                             N),
                 y = N),
             stat = "identity",
             fill = "black",
             color = "black")+
    scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x),
                  labels = trans_format("log10", math_format(10^.x)))+
    annotation_logticks(sides = "b")+
    coord_flip()+
    theme_bw()+
    labs(
        title = paste(prefix.figuretitle,
                      "| Einzelnormen je Periodikum"),
        caption = caption,
        x = "Periodikum",
        y = "Einzelnormen"
    )+
    theme(
        text = element_text(size = 14),
        plot.title = element_text(size = 14,
                                  face = "bold"),
        legend.position = "none",
        plot.margin = margin(10, 20, 10, 10)
    )
```



```{r}
kable(freqtable.normen$periodikum,
      format = "latex",
      align = 'P{3cm}',
      booktabs = TRUE,
      longtable = TRUE,
      col.names = c("Periodikum",
                    "Einzelnormen",
                    "% Gesamt",
                    "% Kumulativ")) %>% kable_styling(latex_options = "repeat_header")

```


\newpage

### Rechtsakte mit veröffentlichtem Normtext




```{r C-DBR_05_Rechtsakte_Barplot_Periodikum, fig.height = 10, fig.width = 8}
ggplot(data = freqtable.rechtsakte$periodikum[-.N]) +
    geom_bar(aes(x = reorder(periodikum,
                             N),
                 y = N),
             stat = "identity",
             fill = "black",
             color = "black") +
    scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x),
                  labels = trans_format("log10", math_format(10^.x)))+
    annotation_logticks(sides = "b")+
    coord_flip()+
    theme_bw()+
    labs(
        title = paste(prefix.figuretitle,
                      "| Rechtsakte mit Normtext je Periodikum"),
        caption = caption,
        x = "Periodikum",
        y = "Rechtsakte"
    )+
    theme(
        text = element_text(size = 14),
        plot.title = element_text(size = 14,
                                  face = "bold"),
        legend.position = "none",
        plot.margin = margin(10, 20, 10, 10)
    )
```


\newpage

```{r}
kable(freqtable.rechtsakte$periodikum,
      format = "latex",
      align = 'P{3cm}',
      booktabs = TRUE,
      longtable = TRUE,
      col.names = c("Periodikum",
                    "Rechtsakte",
                    "% Gesamt",
                    "% Kumulativ")) %>% kable_styling(latex_options = "repeat_header")
```



\newpage




### Alle Rechtsakte (mit und ohne Normtext)


```{r C-DBR_05_Meta_Barplot_Periodikum, fig.height = 10, fig.width = 8}
ggplot(data = freqtable.xmlmeta$periodikum[-.N]) +
    geom_bar(aes(x = reorder(periodikum,
                             N),
                 y = N),
             stat = "identity",
             fill = "black",
             color = "black") +
    scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x),
                  labels = trans_format("log10", math_format(10^.x)))+
    annotation_logticks(sides = "b")+
    coord_flip()+
    theme_bw() +
    labs(
        title = paste(prefix.figuretitle,
                      "| Rechtsakte mit/ohne Normtext je Periodikum"),
        caption = caption,
        x = "Periodikum",
        y = "Rechtsakte"
    )+
    theme(
        text = element_text(size = 14),
        plot.title = element_text(size = 14,
                                  face = "bold"),
        legend.position = "none",
        plot.margin = margin(10, 20, 10, 10)
    )
```



\newpage


```{r}
kable(freqtable.xmlmeta$periodikum,
      format = "latex",
      align = 'P{3cm}',
      booktabs = TRUE,
      longtable = TRUE,
      col.names = c("Periodikum",
                    "Rechtsakte",
                    "% Gesamt",
                    "% Kumulativ")) %>% kable_styling(latex_options = "repeat_header")
```






\newpage

## Nach Ausfertigungsjahr



### Einzelnormen


```{r C-DBR_06_Einzelnormen_Barplot_Ausfertigungsjahr, fig.height = 7, fig.width = 11}

ggplot(data = freqtable.normen$ausfertigung_jahr[-.N][,lapply(.SD, as.numeric)]) +
    geom_bar(aes(x = ausfertigung_jahr,
                 y = N),
             stat = "identity",
             fill = "black")+
    theme_bw()+
    labs(
        title = paste(prefix.figuretitle,
                      "| Einzelnormen je Ausfertigungsjahr"),
        caption = caption,
        x = "Ausfertigungsjahr",
        y = "Einzelnormen"
    )+
    theme(
        text = element_text(size = 16),
        plot.title = element_text(size = 16,
                                  face = "bold"),
        legend.position = "none",
        plot.margin = margin(10, 20, 10, 10)
    )

```


```{r}

kable(freqtable.normen$ausfertigung_jahr,
      format = "latex",
      align = 'P{3cm}',
      booktabs = TRUE,
      longtable = TRUE,
      col.names = c("Jahr",
                    "Einzelnormen",
                    "% Gesamt",
                    "% Kumulativ")) %>% kable_styling(latex_options = "repeat_header")

```


\newpage

### Rechtsakte mit veröffentlichtem Normtext


```{r C-DBR_06_Rechtsakte_Barplot_Ausfertigungsjahr, fig.height = 7, fig.width = 11}
ggplot(data = freqtable.rechtsakte$ausfertigung_jahr[-.N][,lapply(.SD, as.numeric)]) +
    geom_bar(aes(x = ausfertigung_jahr,
                 y = N),
             stat = "identity",
             fill = "black") +
    theme_bw()+
    labs(
        title = paste(prefix.figuretitle,
                      "| Rechtsakte mit Normtext je Ausfertigungsjahr"),
        caption = caption,
        x = "Ausfertigungsjahr",
        y = "Rechtsakte"
    )+
    theme(
        text = element_text(size = 16),
        plot.title = element_text(size = 16,
                                  face = "bold"),
        legend.position = "none",
        plot.margin = margin(10, 20, 10, 10)
    )

```


```{r}

kable(freqtable.rechtsakte$ausfertigung_jahr,
      format = "latex",
      align = 'P{3cm}',
      booktabs = TRUE,
      longtable = TRUE,
      col.names = c("Jahr",
                    "Rechtsakte",
                    "% Gesamt",
                    "% Kumulativ")) %>% kable_styling(latex_options = "repeat_header")

```



\newpage

###  Alle Rechtsakte (mit und ohne Normtext)


```{r C-DBR_06_Meta_Barplot_Ausfertigungsjahr, fig.height = 7, fig.width = 11}
ggplot(data = freqtable.xmlmeta$ausfertigung_jahr[-.N][,lapply(.SD, as.numeric)]) +
    geom_bar(aes(x = ausfertigung_jahr,
                 y = N),
             stat = "identity",
             fill = "black") +
    theme_bw()+
    labs(
        title = paste(prefix.figuretitle,
                      "| Rechtsakte mit/ohne Normtext je Ausfertigungsjahr"),
        caption = caption,
        x = "Ausfertigungsjahr",
        y = "Rechtsakte"
    )+
    theme(
        text = element_text(size = 16),
        plot.title = element_text(size = 16,
                                  face = "bold"),
        legend.position = "none",
        plot.margin = margin(10, 20, 10, 10)
    )
```



```{r}

kable(freqtable.xmlmeta$ausfertigung_jahr,
      format = "latex",
      align = 'P{3cm}',
      booktabs = TRUE,
      longtable = TRUE,
      col.names = c("Jahr",
                    "Rechtsakte",
                    "% Gesamt",
                    "% Kumulativ")) %>% kable_styling(latex_options = "repeat_header")

```



# Dateigrößen


```{r, C-DBR_07_Density_Dateigroessen_XML, fig.height = 6, fig.width = 9}


xml.MB <- file.size(files.xml) / 10^6


dt.plot <- data.table(xml.MB)

ggplot(data = dt.plot,
       aes(x = xml.MB)) +
    geom_density(fill = "black") +
    scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
                  labels = trans_format("log10", math_format(10^.x)))+
    annotation_logticks(sides = "b")+
    theme_bw() +
    labs(
        title = paste(prefix.figuretitle,
                      "| Verteilung der Dateigrößen (XML)"),
        caption = caption,
        x = "Dateigröße in MB",
        y = "Dichte"
    )+
    theme(
        text = element_text(size = 14),
        plot.title = element_text(size = 14,
                                  face = "bold"),
        legend.position = "none",
        panel.spacing = unit(0.1, "lines"),
        plot.margin = margin(10, 20, 10, 10)
    )
    
```




```{r, C-DBR_07_Density_Dateigroessen_PDF, fig.height = 6, fig.width = 9}


pdf.MB <- file.size(files.pdf) / 10^6


dt.plot <- data.table(pdf.MB)

ggplot(data = dt.plot,
       aes(x = pdf.MB)) +
    geom_density(fill = "black") +
    scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
                  labels = trans_format("log10", math_format(10^.x)))+
    annotation_logticks(sides = "b")+
    theme_bw() +
    labs(
        title = paste(prefix.figuretitle,
                      "| Verteilung der Dateigrößen (PDF)"),
        caption = caption,
        x = "Dateigröße in MB",
        y = "Dichte"
    )+
    theme(
        text = element_text(size = 14),
        plot.title = element_text(size = 14,
                                  face = "bold"),
        legend.position = "none",
        panel.spacing = unit(0.1, "lines"),
        plot.margin = margin(10, 20, 10, 10)
    )
    
```



```{r, C-DBR_07_Density_Dateigroessen_TXT, fig.height = 6, fig.width = 9}


txt.MB <- file.size(files.txt) / 10^6


dt.plot <- data.table(txt.MB)

ggplot(data = dt.plot,
       aes(x = txt.MB)) +
    geom_density(fill = "black") +
    scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
                  labels = trans_format("log10", math_format(10^.x)))+
    annotation_logticks(sides = "b")+
    theme_bw() +
    labs(
        title = paste(prefix.figuretitle,
                      "| Verteilung der Dateigrößen (TXT)"),
        caption = caption,
        x = "Dateigröße in MB",
        y = "Dichte"
    )+
    theme(
        text = element_text(size = 14),
        plot.title = element_text(size = 14,
                                  face = "bold"),
        legend.position = "none",
        panel.spacing = unit(0.1, "lines"),
        plot.margin = margin(10, 20, 10, 10)
    )
    
```




```{r, C-DBR_07_Density_Dateigroessen_EPUB, fig.height = 6, fig.width = 9}


epub.MB <- file.size(files.epub) / 10^6


dt.plot <- data.table(epub.MB)

ggplot(data = dt.plot,
       aes(x = epub.MB)) +
    geom_density(fill = "black") +
    scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
                  labels = trans_format("log10", math_format(10^.x)))+
    annotation_logticks(sides = "b")+
    theme_bw() +
    labs(
        title = paste(prefix.figuretitle,
                      "| Verteilung der Dateigrößen (EPUB)"),
        caption = caption,
        x = "Dateigröße in MB",
        y = "Dichte"
    )+
    theme(
        text = element_text(size = 14),
        plot.title = element_text(size = 14,
                                  face = "bold"),
        legend.position = "none",
        panel.spacing = unit(0.1, "lines"),
        plot.margin = margin(10, 20, 10, 10)
    )
    
```




# Kryptographische Signaturen


## Zwei-Phasen-Signatur

Die Integrität und Echtheit der einzelnen Archive des Datensatzes sind durch eine Zwei-Phasen-Signatur sichergestellt.

In **Phase I** werden während der Kompilierung für jedes ZIP-Archiv, das Codebook und die Robustness Checks Hash-Werte in zwei verschiedenen Verfahren (SHA2-256 und SHA3-512) berechnet und in einer CSV-Datei dokumentiert.

In **Phase II** werden diese CSV-Datei und der Compilation Report mit meinem persönlichen geheimen GPG-Schlüssel signiert. Dieses Verfahren stellt sicher, dass die Kompilierung von jedermann durchgeführt werden kann, insbesondere im Rahmen von Replikationen, die persönliche Gewähr für Ergebnisse aber dennoch vorhanden bleibt.


## Persönliche GPG-Signatur

Die während der Kompilierung des Datensatzes erstellte CSV-Datei mit den Hash-Prüfsummen und der Compilation Report sind mit meiner persönlichen GPG-Signatur versehen. Der mit dieser Version korrespondierende Public Key ist sowohl mit dem Datensatz als auch mit dem Source Code hinterlegt. Er hat folgende Kenndaten:
 
 **Name:** Sean Fobbe (fobbe-data@posteo.de)
 
 **Fingerabdruck:** FE6F B888 F0E5 656C 1D25  3B9A 50C4 1384 F44A 4E42





\newpage









```{r, results = "asis"}
cat(readLines(tar_read(changelog)),
    sep = "\n")

```







# Parameter für strenge Replikationen


```{r}
system2("openssl", "version", stdout = TRUE)

sessionInfo()

```


# Literaturverzeichnis
